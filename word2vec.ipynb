{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把数据初始化的seed约定好,尽可能让每次复现都很类似\n",
    "## 我喜欢用769，131，1331\n",
    "random.seed(769)\n",
    "np.random.seed(769)\n",
    "torch.manual_seed(769)\n",
    "\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(769)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置一些超参数\n",
    "# 周围单词，几个算周围单词\n",
    "c = 3\n",
    "k = 100 # nagative samples\n",
    "NUM_EPOCHS = 2\n",
    "MAX_VOCAB_SIZE = 30000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.2\n",
    "EMBEDDING_SIZE = 100\n",
    "LOG_FILE = \"word_embedding.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理一下文章的输入，因为text8已经处理好了标点和一些额外空格，所以直接split就ok\n",
    "def word_tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个单词表，只构建常用的单词，不常用的用一个标记表示\n",
    "with open(\"text8.train.txt\",\"r\") as fin:\n",
    "    text = fin.read()\n",
    "# text[:100]\n",
    "text = [w for w in word_tokenize(text)] \n",
    "vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1)) # 拿到标准的3w - 1个，不常见的都标记为unk\n",
    "vocab[\"<unk>\"] = len(text) - np.sum(list(vocab.values())) # 拿到unk的词频\n",
    "# text[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建双射\n",
    "\n",
    "idx_to_word = [word for word in vocab.keys()]\n",
    "word_to_idx = {word:i for i,word in enumerate(idx_to_word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = np.array([count for count in vocab.values()],dtype = np.float32)\n",
    "word_freqs = word_counts / np.sum(word_counts) # 计算出词频（论文中提到了要提升到 ** 3./4.然后重新normal一次）\n",
    "word_freqs = word_freqs ** (3./4.)\n",
    "word_freqs = word_freqs / np.sum(word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(idx_to_word)\n",
    "VOCAB_SIZE = len(idx_to_word)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现dataloader\n",
    "# __len__ function需要返回整个数据集中有多少个item\n",
    "# __get__ 根据给定的index返回一个item\n",
    "\n",
    "class WordEmbeddingDataset(tud.Dataset):\n",
    "    def __init__(self,text,word_to_idx,idx_to_word,word_freqs,word_counts):\n",
    "        \n",
    "        super(WordEmbeddingDataset,self).__init__()\n",
    "         #字典 get() 函数返回指定键的值（第一个参数），如果值不在字典中返回默认值（第二个参数）\n",
    "        self.text_encoded = [word_to_idx.get(word,VOCAB_SIZE-1) for word in text]\n",
    "        \n",
    "        self.text_encoded = torch.Tensor(self.text_encoded).long()\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "        self.word_counts = torch.Tensor(word_counts)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        center_word = self.text_encoded[idx]\n",
    "        pos_indices = list(range(idx - c,idx)) + list(range(idx + 1,idx + c + 1)) \n",
    "        #可能超出文本的话,防止越界\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices]\n",
    "        # 窗口中出现的单词\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        \n",
    "        # 产生一些周围没有的单词\n",
    "        # 负例采样单词索引，torch.multinomial作用是对self.word_freqs做K * pos_words.shape[0]次取值，输出的是self.word_freqs对应的下标。\n",
    "        # 取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大。\n",
    "        # 每个正确的单词采样K个，pos_words.shape[0]是正确单词数量\n",
    "        neg_words = torch.multinomial(self.word_freqs,k * pos_words.shape[0],True)\n",
    "        \n",
    "        return center_word,pos_words,neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordEmbeddingDataset(text,word_to_idx,idx_to_word,word_freqs,word_counts)\n",
    "#win32下只能num_workers只能使用默认值\n",
    "dataloader = tud.DataLoader(dataset,batch_size=BATCH_SIZE,shuffle = True,num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,(center_word, pos_words, neg_words ) in enumerate(dataloader):\n",
    "#     print(center_word, pos_words, neg_words)\n",
    "#     if i > 1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size):\n",
    "        super(EmbeddingModel,self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        initrange =  0.5 / self.embed_size\n",
    "        self.out_embed = nn.Embedding(self.vocab_size,self.embed_size,sparse = False)\n",
    "        self.out_embed.weight.data.uniform_(-initrange,initrange)\n",
    "        \n",
    "        self.in_embed = nn.Embedding(self.vocab_size,self.embed_size,sparse = False)\n",
    "        self.in_embed.weight.data.uniform_(-initrange,initrange)\n",
    "        \n",
    "    def forward(self,input_labels,pos_labels,neg_labels):\n",
    "        batch_size = input_labels.size(0)\n",
    "        \n",
    "        input_embedding = self.in_embed(input_labels) # B * embed_size\n",
    "        pos_embedding =  self.out_embed(pos_labels) # B * (2*C) * embed_size\n",
    "        neg_embedding =  self.out_embed(neg_labels)  # B * (2*C * K) * embed_size\n",
    "        \n",
    "        log_pos = torch.bmm(pos_embedding, input_embedding.unsqueeze(2)).squeeze()  # B * (2*C)\n",
    "        log_neg = torch.bmm(neg_embedding, -input_embedding.unsqueeze(2)).squeeze()   # B * (2*C*K)\n",
    "        \n",
    "        log_pos = F.logsigmoid(log_pos).sum(1)\n",
    "        log_neg = F.logsigmoid(log_neg).sum(1)\n",
    "        \n",
    "        loss = log_neg + log_pos\n",
    "        \n",
    "        return -loss\n",
    "    \n",
    "    def input_embeddings(self):\n",
    "        return self.in_embed.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingModel(VOCAB_SIZE,EMBEDDING_SIZE)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "## 训练模型\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = LEARNING_RATE)\n",
    "\n",
    "\n",
    "#大概需要5小时才能收敛\n",
    "for e in range(NUM_EPOCHS):\n",
    "    for i,(input_labels, pos_labels, neg_labels) in enumerate(dataloaders):\n",
    "        #转为long保险起见\n",
    "        input_labels = input_labels.long()\n",
    "        pos_labels = pos_labels.long()\n",
    "        neg_labels = neg_labels.long()\n",
    "        if USE_CUDA:\n",
    "            input_labels = input_labels.cuda()\n",
    "            pos_labels = pos_labels.cuda()\n",
    "            neg_labels = neg_labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        #返回若干个loss，对他求平均\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            with open(LOG_FILE, \"a\") as fout:\n",
    "                fout.write(\"epoch: {}, iter: {}, loss: {}\\n\".format(e, i, loss.item()))\n",
    "                print(\"epoch: {}, iter: {}, loss: {}\".format(e, i, loss.item()))\n",
    "        \n",
    "        \n",
    "        embedding_weights = model.input_embeddings()\n",
    "        np.save(\"embedding-{}\".format(EMBEDDING_SIZE), embedding_weights)\n",
    "        torch.save(model.state_dict(),\"embedding-{}.th\".format(EMBEDDING_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型的评估\n",
    "def evaluate(filename, embedding_weights): \n",
    "    if filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(filename, sep=\",\")\n",
    "    else:\n",
    "        data = pd.read_csv(filename, sep=\"\\t\")\n",
    "    human_similarity = []\n",
    "    model_similarity = []\n",
    "    for i in data.iloc[:, 0:2].index:\n",
    "        word1, word2 = data.iloc[i, 0], data.iloc[i, 1]\n",
    "        if word1 not in word_to_idx or word2 not in word_to_idx:\n",
    "            continue\n",
    "        else:\n",
    "            word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]\n",
    "            word1_embed, word2_embed = embedding_weights[[word1_idx]], embedding_weights[[word2_idx]]\n",
    "            model_similarity.append(float(sklearn.metrics.pairwise.cosine_similarity(word1_embed, word2_embed)))\n",
    "            human_similarity.append(float(data.iloc[i, 2]))\n",
    "\n",
    "    return scipy.stats.spearmanr(human_similarity, model_similarity)# , model_similarity\n",
    "    \n",
    "def find_nearest(word):\n",
    "    index = word_to_idx[word]\n",
    "    embedding = embedding_weights[index]\n",
    "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "    return [idx_to_word[i] for i in cos_dis.argsort()[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in [\"good\", \"fresh\", \"monster\", \"green\", \"like\", \"america\", \"chicago\", \"work\", \"computer\", \"language\"]:\n",
    "    print(word, find_nearest(word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit4119a9282d8f463ba3612d7491e7e822"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
